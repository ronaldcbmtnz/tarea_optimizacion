{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671b521c",
   "metadata": {},
   "source": [
    "# Optimización de f(x,y) = -tan²(0.5(sin x + sin y))\n",
    "\n",
    "Este notebook permite ejecutar **individualmente cada método de optimización**:\n",
    "\n",
    "- Newton con paso fijo\n",
    "- Newton amortiguado (damped) con búsqueda lineal\n",
    "- BFGS\n",
    "\n",
    "Al ejecutar la celda correspondiente a un método, se muestran:\n",
    "\n",
    "1. Convergencia de f(x,y) vs iteración\n",
    "2. Trayectorias sobre contornos de la función\n",
    "3. Valor final y número de iteraciones\n",
    "\n",
    "Puedes cambiar el punto inicial dentro de cada celda para probar distintas condiciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b549c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Definición de la función objetivo ===\n",
    "def f(x):\n",
    "    u = 0.5*(np.sin(x[0]) + np.sin(x[1]))\n",
    "    return -np.tan(u)**2\n",
    "\n",
    "# === Gradiente ===\n",
    "def grad_f(x):\n",
    "    u = 0.5*(np.sin(x[0]) + np.sin(x[1]))\n",
    "    sec2 = 1 / np.cos(u)**2\n",
    "    du_dx = 0.5*np.cos(x[0])\n",
    "    du_dy = 0.5*np.cos(x[1])\n",
    "    return -2*np.tan(u)*sec2*np.array([du_dx, du_dy])\n",
    "\n",
    "# === Hessiana ===\n",
    "def hess_f(x):\n",
    "    u = 0.5*(np.sin(x[0]) + np.sin(x[1]))\n",
    "    sec2 = 1/np.cos(u)**2\n",
    "    tan_u = np.tan(u)\n",
    "    du_dx = 0.5*np.cos(x[0])\n",
    "    du_dy = 0.5*np.cos(x[1])\n",
    "    d2u_dx2 = -0.5*np.sin(x[0])\n",
    "    d2u_dy2 = -0.5*np.sin(x[1])\n",
    "    H = np.zeros((2,2))\n",
    "    d2f_du2 = -2*(sec2**2)*(1 + 2*tan_u**2)\n",
    "    df_du = -2*tan_u*sec2\n",
    "    for i, (du_i, d2u_i2) in enumerate(zip([du_dx, du_dy], [d2u_dx2, d2u_dy2])):\n",
    "        H[i,i] = d2f_du2*du_i**2 + df_du*d2u_i2\n",
    "    H[0,1] = H[1,0] = d2f_du2*du_dx*du_dy\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ea49cc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Método: Newton (paso fijo α=1.0) — CONVERGIÓ\n",
      "Punto final: [98.9601686 98.9601686]\n",
      "f* = -2.425519\n",
      "Iteraciones: 2\n"
     ]
    }
   ],
   "source": [
    "def newton_fixed_robust(x0, alpha=1.0, tol=1e-6, max_iter=100):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    g = grad_f(x)\n",
    "    tray = []\n",
    "\n",
    "    # Convergencia inicial\n",
    "    if np.linalg.norm(g) < tol:\n",
    "        tray.append(x.copy())\n",
    "        print(f\"\\nMétodo: Newton (paso fijo α={alpha}) — CONVERGIÓ\")\n",
    "        print(f\"Punto final: {x}\")\n",
    "        print(f\"f* = {f(x):.6f}\")\n",
    "        print(f\"Iteraciones: 0\")\n",
    "        return\n",
    "\n",
    "    tray.append(x.copy())\n",
    "    diverged = False\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        try:\n",
    "            H = hess_f(x)\n",
    "            H += 1e-6*np.eye(2)\n",
    "            p = -np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f\"⚠️ Hessiana singular en iter {k}\")\n",
    "            diverged = True\n",
    "            break\n",
    "\n",
    "        # Limitar tamaño del paso\n",
    "        if np.linalg.norm(p) > 1.0:\n",
    "            p = p / np.linalg.norm(p)\n",
    "\n",
    "        x = x + alpha*p\n",
    "        tray.append(x.copy())\n",
    "        g = grad_f(x)\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "        fx_new = f(x)\n",
    "        if np.isnan(fx_new) or np.isinf(fx_new) or np.linalg.norm(x) > 1e6:\n",
    "            diverged = True\n",
    "            break\n",
    "\n",
    "    estado = \"DIVERGIÓ\" if diverged else \"CONVERGIÓ\"\n",
    "    print(f\"\\nMétodo: Newton (paso fijo α={alpha}) — {estado}\")\n",
    "    print(f\"Punto final: {x}\")\n",
    "    print(f\"f* = {f(x):.6f}\")\n",
    "    print(f\"Iteraciones: {len(tray)-1}\")  # restamos el punto inicial\n",
    "\n",
    "    \n",
    "newton_fixed_robust([99,99])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7e33f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Método: Newton Damped — CONVERGIÓ\n",
      "Punto final: [0.         3.14159265]\n",
      "f* = -0.000000\n",
      "Iteraciones: 0\n"
     ]
    }
   ],
   "source": [
    "def newton_damped_robust(x0, tol=1e-6, max_iter=100):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    g = grad_f(x)\n",
    "    tray = []\n",
    "\n",
    "    if np.linalg.norm(g) < tol:\n",
    "        tray.append(x.copy())\n",
    "        print(f\"\\nMétodo: Newton Damped — CONVERGIÓ\")\n",
    "        print(f\"Punto final: {x}\")\n",
    "        print(f\"f* = {f(x):.6f}\")\n",
    "        print(f\"Iteraciones: 0\")\n",
    "        return\n",
    "\n",
    "    tray.append(x.copy())\n",
    "    diverged = False\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        try:\n",
    "            H = hess_f(x)\n",
    "            H += 1e-6*np.eye(2)\n",
    "            p = -np.linalg.solve(H, g)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f\"⚠️ Hessiana singular en iter {k}\")\n",
    "            diverged = True\n",
    "            break\n",
    "\n",
    "        if np.linalg.norm(p) > 1.0:\n",
    "            p = p / np.linalg.norm(p)\n",
    "\n",
    "        # Backtracking line search\n",
    "        alpha = 1.0\n",
    "        while f(x + alpha*p) > f(x) + 1e-4*alpha*np.dot(g, p):\n",
    "            alpha *= 0.5\n",
    "            if alpha < 1e-6:\n",
    "                diverged = True\n",
    "                break\n",
    "\n",
    "        x = x + alpha*p\n",
    "        tray.append(x.copy())\n",
    "        g = grad_f(x)\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "        fx_new = f(x)\n",
    "        if np.isnan(fx_new) or np.isinf(fx_new) or np.linalg.norm(x) > 1e6:\n",
    "            diverged = True\n",
    "            break\n",
    "\n",
    "    estado = \"DIVERGIÓ\" if diverged else \"CONVERGIÓ\"\n",
    "    print(f\"\\nMétodo: Newton Damped — {estado}\")\n",
    "    print(f\"Punto final: {x}\")\n",
    "    print(f\"f* = {f(x):.6f}\")\n",
    "    print(f\"Iteraciones: {len(tray)-1}\")\n",
    "\n",
    "    \n",
    "newton_damped_robust([0,np.pi])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "68b087d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Método: BFGS — CONVERGIÓ\n",
      "Punto final: [1.57079634 1.57079635]\n",
      "f* = -2.425519\n",
      "Iteraciones: 8\n"
     ]
    }
   ],
   "source": [
    "def bfgs_robust(x0, tol=1e-6, max_iter=100):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    g = grad_f(x)\n",
    "    tray = []\n",
    "\n",
    "    if np.linalg.norm(g) < tol:\n",
    "        tray.append(x.copy())\n",
    "        print(f\"\\nMétodo: BFGS — CONVERGIÓ\")\n",
    "        print(f\"Punto final: {x}\")\n",
    "        print(f\"f* = {f(x):.6f}\")\n",
    "        print(f\"Iteraciones: 0\")\n",
    "        return\n",
    "\n",
    "    tray.append(x.copy())\n",
    "    B = np.eye(2)\n",
    "    diverged = False\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        p = -np.linalg.solve(B, g)\n",
    "\n",
    "        if np.linalg.norm(p) > 1.0:\n",
    "            p = p / np.linalg.norm(p)\n",
    "\n",
    "        # Backtracking line search\n",
    "        alpha = 1.0\n",
    "        while f(x + alpha*p) > f(x) + 1e-4*alpha*np.dot(g, p):\n",
    "            alpha *= 0.5\n",
    "            if alpha < 1e-6:\n",
    "                diverged = True\n",
    "                break\n",
    "\n",
    "        s = alpha*p\n",
    "        x_new = x + s\n",
    "        fx_new = f(x_new)\n",
    "\n",
    "        if np.isnan(fx_new) or np.isinf(fx_new) or np.linalg.norm(x_new) > 1e6:\n",
    "            diverged = True\n",
    "            break\n",
    "\n",
    "        y = grad_f(x_new) - g\n",
    "        if np.dot(y, s) > 1e-10:\n",
    "            Bs = B @ s\n",
    "            B = B + np.outer(y, y)/np.dot(y, s) - np.outer(Bs, Bs)/np.dot(s, Bs)\n",
    "\n",
    "        x = x_new\n",
    "        g = grad_f(x)\n",
    "        tray.append(x.copy())\n",
    "\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "\n",
    "    estado = \"DIVERGIÓ\" if diverged else \"CONVERGIÓ\"\n",
    "    print(f\"\\nMétodo: BFGS — {estado}\")\n",
    "    print(f\"Punto final: {x}\")\n",
    "    print(f\"f* = {f(x):.6f}\")\n",
    "    print(f\"Iteraciones: {len(tray)-1}\")\n",
    "\n",
    "    \n",
    "bfgs_robust([0.1,2])    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
